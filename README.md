# Im2Sim: Inferring and coding simulations of physical systems from  photographs of visual phenomena

This code use VLMS (GPT/Qwen/Gemini…) to look at real images of visual patterns (clouds, waves, city) , identify the model of the physical system beyond the pattern, write it as a generative code, and run this code to generate a simulated image of the pattern. Basically, identify the physical/biological/chemical/social system beyond the structure that appears in the image. Write a simulation/model of these systems (as python code) and run this code to generate simulated  images.
More details on the method can be found in the document: [SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art](https://arxiv.org/pdf/2511.01817).


This code have to versions: one simple with everything contained in one file and a second more advanced. The more advanced version was used in the paper Im2Sim, but the simple one is much easier to use and understand and faster in generating images.



​![](/Scheme.jpg)
<small>__Scheme of the Im2Sim2Im pipeline, the VLM receive an image and asks to identify the process/system in the image and write a simulation of this system, it then run the code to generate a simulated image.__</small>
# How to use simple version (
## Use Generate_Texture_From_Image_Simple.py for generating texture from referance texture image or  Generative_From_image_Simple.py for replicating any image 


1. Purchase tokens and get an API key  from [openrouter](https://openrouter.ai/docs/api/reference/authentication) (Basically universal api for all major LLMs)

2. Set the api key in API_KEY
3. Set folder of input images in InDir (this will be use as reference for the generated images)
4. Set output folder where the generative code will be saved
5. You can update the model at model paramater from the [list of models availble at open router](https://openrouter.ai/models) (note that it most be vision capable model)

# How to use advance version:

## 1. Update your preferred API key at API_KEYS.py.  

​The code relies on API for major AI LVLM suppliers (GPT, Gemini, Qwen, DeepSeek, Claude, Grok...). You need to have API keys (and purchase tokens) for at least one of those (GPT-5  is recommended).

The code currently supports APIs from OpenAI, Together.AI (Qwen,DeepSeek), Grok, Gemini, and Claude. You can add other APIs to tools/VisualQuestion.py.

## 2. Run the script to generate a simulation from an image.​

The two main scripts are:

***Im2Sim2Im_from_image_2_code_2_im.py***  and  ***Im2Sim2Im_from_image_2_code_2_multi_im.py***

These two scripts are basically identical, with the exception that the last will generate multiple images per  simulation and the former will only generate a single image.

In the script  **__main__** section set input parameters:

**model** = select model name(for example gpt-5)​

**real_im_dir** = Input folder containing photos of real world phenomena. The AI/VLM will look at this image and will try to identify  system that formed it and write a simulation of this system. Reference folder is supplied at **"sample_images"**

**main_outdir** = The path to the output folder where the simulation code and images will be saved. THIS has to be inside the code folder, and the path MUST be given as a relative path to the code folder (this folder will contain some of the generated scripts, which will be imported into the code in real time and run).

Note that the code should run as is with the sample input/output folder without any a changes (accept adding api key).

*** The script will generate code fsimulation, and will run it without asking for permission; it will also install packages using pip without asking permission.***​

### Output structure:

The output folder will contain generated models and images. One folder per model with a folder name that matches the input image.​

The simulated  images generated by the model will appear in the **“images”** subdirectory or in **image.jpg** file in the model folder.

The script used to simulate the system and generate the images will appear at:

**“generate.py“**  in the model dir.

”description” file will contain a description of the  model/simulation.

​

​![](/Results.jpg)
<small>__Some examples of the results, top real photo, bottom image that was generated  by the simulation of the system in the real photo__</small>


## Testing generated models accuracy
Testing general model accuracy by matching the real image and the generated image to each other in multiple-choice test can be done using the script in testing/natural2simulated_image_matching.py
For more details, see the paper.

# Links
[The SciTextures dataset 1270 generative codes and simulations, and 100,000 images generates by these scripts](https://sites.google.com/view/scitextures/home), [ZENODO](https://zenodo.org/records/17485502)

[Sampled natural images that were used as input for the script](https://drive.google.com/drive/folders/1iNywku9Om4mYR9RnBVarUQd4fSBjJ2-V?usp=sharing)

[Example responses from various of models](https://drive.google.com/drive/folders/19Sfib1XAfaJYhYysVASup9cvxWHAQn_u?usp=sharing)


​
